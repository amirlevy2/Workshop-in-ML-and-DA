{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":52784,"databundleVersionId":5687476,"sourceType":"competition"},{"sourceId":9846552,"sourceType":"datasetVersion","datasetId":6041405},{"sourceId":9846576,"sourceType":"datasetVersion","datasetId":6041426}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false},"papermill":{"default_parameters":{},"duration":167.808578,"end_time":"2024-05-18T13:33:45.687121","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-05-18T13:30:57.878543","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tabpfn --no-index --find-links=file:////kaggle/input/tabpfn2/tabpfn-0.1.9-py3-none-any.whl\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T07:31:41.38528Z","iopub.execute_input":"2024-11-16T07:31:41.385777Z","iopub.status.idle":"2024-11-16T07:31:54.816768Z","shell.execute_reply.started":"2024-11-16T07:31:41.385729Z","shell.execute_reply":"2024-11-16T07:31:54.814991Z"},"papermill":{"duration":13.160404,"end_time":"2024-05-18T13:31:13.722216","exception":false,"start_time":"2024-05-18T13:31:00.561812","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!mkdir -p /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff\n\n!cp /kaggle/input/checkpoint/prior_diff_real_checkpoint_n_0_epoch_100.cpkt /opt/conda/lib/python3.10/site-packages/tabpfn/models_diff/","metadata":{"execution":{"iopub.status.busy":"2024-11-16T07:31:57.935393Z","iopub.execute_input":"2024-11-16T07:31:57.93592Z","iopub.status.idle":"2024-11-16T07:32:01.339988Z","shell.execute_reply.started":"2024-11-16T07:31:57.935874Z","shell.execute_reply":"2024-11-16T07:32:01.338454Z"},"papermill":{"duration":2.895342,"end_time":"2024-05-18T13:31:16.621319","exception":false,"start_time":"2024-05-18T13:31:13.725977","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from catboost import CatBoostClassifier\nimport pandas as pd\nimport numpy as np\nimport xgboost\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold\nfrom lightgbm import LGBMClassifier as lgb\nimport tabpfn\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom datetime import datetime\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T07:32:03.543385Z","iopub.execute_input":"2024-11-16T07:32:03.543889Z","iopub.status.idle":"2024-11-16T07:32:07.921027Z","shell.execute_reply.started":"2024-11-16T07:32:03.543843Z","shell.execute_reply":"2024-11-16T07:32:07.919512Z"},"papermill":{"duration":7.029856,"end_time":"2024-05-18T13:31:23.654919","exception":false,"start_time":"2024-05-18T13:31:16.625063","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/train.csv')\n\ngreeks=pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/greeks.csv')\n\ntimes = greeks['Epsilon'].copy()\n\ntimes[greeks['Epsilon'] != 'Unknown'] = greeks['Epsilon'][greeks['Epsilon'] != 'Unknown'].map(lambda x: datetime.strptime(x,'%m/%d/%Y').toordinal())\n\ntimes[greeks['Epsilon'] == 'Unknown'] = np.nan\n\ndf['time']= times\n\ndf = df.dropna(subset = ['time']).reset_index(drop=True)\n\ndf.drop('Id', axis=1, inplace=True)\n# df.drop('time', axis=1, inplace=True)\n\nbad_cols=['AF', 'AH', 'AR', 'AX', 'BC', 'CF', 'CS', 'CU', 'CW ', 'DI', 'EE', 'EH', 'EJ', 'EL', 'FI','GI']\nbad_cols=[ 'AR', 'AX', 'BC', 'CF', 'CS', 'CU', 'CW ', 'DI', 'EE', 'EH', 'EJ', 'EL', 'FI','GI']\n\ntrain_df = df.drop('Class', axis=1)\n\ntrain_df = train_df.drop(columns=bad_cols)\n\ndef remove_var(X_train, isVar, per = 0.1):\n    # Convert all columns in X_train to numeric, coercing errors to NaN if any non-numeric data is present\n    X_train_ = X_train.apply(pd.to_numeric, errors='coerce')\n    \n    # Calculate the variance of each column, ensuring that variances are in numeric format\n    variances = X_train_.var().astype(float)\n    \n    # Sort variances in descending order and select the top percentage most variable columns\n    top_var = int(len(variances) * per)\n    \n    high_variance_columns = variances.nlargest(top_var).index\n    \n    if not isVar:\n        # Drop these high variance columns from the train_df\n        X_train.drop(columns=high_variance_columns, inplace=True)\n    else: \n        X_train = X_train[high_variance_columns]\n    \n    return X_train, high_variance_columns\n\n# train_df, high_variance_columns = remove_var(train_df, isVar=False, per=0.1)\n\ntest_df = df['Class']\nscale_pos = (test_df == 0).sum() / (test_df == 1).sum()\nscale_pos\n# change B values to 1 and A values to 0\n# plt.figure(figsize=(15, 10))\n# sns.boxplot(data=df, orient=\"h\", palette=\"Set2\")\n# plt.title(\"Boxplots for All Columns\")\n# plt.xlabel(\"Values\")\n# plt.ylabel(\"Columns\")\n# plt.show()\n#print(train_df.shape)\n\n#train_df = train_df.dropna(subset=['time'])\n\n#print(train_df.shape)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-16T07:32:10.855415Z","iopub.execute_input":"2024-11-16T07:32:10.856151Z","iopub.status.idle":"2024-11-16T07:32:10.938589Z","shell.execute_reply.started":"2024-11-16T07:32:10.856105Z","shell.execute_reply":"2024-11-16T07:32:10.93741Z"},"papermill":{"duration":0.074716,"end_time":"2024-05-18T13:31:23.733409","exception":false,"start_time":"2024-05-18T13:31:23.658693","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_df_1 = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\ntest_df_1.drop('Id', axis=1, inplace=True)\ntest_df_1 = test_df_1.apply(pd.to_numeric, errors='coerce')\ntest_df_1 = test_df_1.drop(columns=bad_cols)\ntest_df_1.fillna(test_df_1.mean(), inplace=True)\n# test_df_1.drop(columns=high_variance_columns, inplace=True)\n\ntest_df_1['time'] = max(times)+1\n\n# scale = StandardScaler().fit(test_df_1)\n\n# test_df_1 = scale.transform(test_df_1)\n\nprint((test_df_1))\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:23:17.392229Z","iopub.execute_input":"2024-11-15T18:23:17.392593Z","iopub.status.idle":"2024-11-15T18:23:17.452101Z","shell.execute_reply.started":"2024-11-15T18:23:17.392553Z","shell.execute_reply":"2024-11-15T18:23:17.450931Z"},"papermill":{"duration":0.048553,"end_time":"2024-05-18T13:31:23.785754","exception":false,"start_time":"2024-05-18T13:31:23.737201","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pt = PowerTransformer()\ncols=train_df.columns\ntrain_df=pt.fit_transform(train_df,)\ntest_df_1=pt.transform(test_df_1)\ntrain_df=pd.DataFrame(train_df,columns=cols)\ntest_df_1=pd.DataFrame(test_df_1,columns=cols)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# replace null values with the mean of the column\ntrain_df = train_df.apply(pd.to_numeric, errors='coerce')\n\ntrain_df.fillna(train_df.mean(), inplace=True)\n\nscale = StandardScaler().fit(train_df)\n\ntrain_df = scale.transform(train_df)","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:23:17.453517Z","iopub.execute_input":"2024-11-15T18:23:17.454003Z","iopub.status.idle":"2024-11-15T18:23:17.501275Z","shell.execute_reply.started":"2024-11-15T18:23:17.453961Z","shell.execute_reply":"2024-11-15T18:23:17.500126Z"},"papermill":{"duration":0.037991,"end_time":"2024-05-18T13:31:23.827483","exception":false,"start_time":"2024-05-18T13:31:23.789492","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# function used to get score in competetion\n\ndef balanced_log_loss(y_true, y_pred):\n\n    # Extracting class labels from y_true\n\n    y_true = y_true.astype(int)\n\n    \n\n    # Computing the number of observations for each class\n\n    N0 = np.sum(y_true == 0)\n\n    N1 = np.sum(y_true == 1)\n\n    \n\n    # Calculating the inverse prevalence weights\n\n    w0 = 1 / N0\n\n    w1 = 1 / N1\n\n    \n\n    # Rescaling the predicted probabilities\n\n    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n\n    y_pred /= y_pred.sum(axis=1, keepdims=True)\n\n    \n\n    # Calculating the logarithmic loss for each class\n\n    log_loss_0 = np.sum((1-y_true) * np.log(y_pred[:, 0])) / N0\n\n    log_loss_1 = np.sum(y_true * np.log(y_pred[:, 1])) / N1\n\n    \n\n    # Computing the balanced logarithmic loss\n\n    balanced_log_loss = (-w0 * log_loss_0 - w1 * log_loss_1)/(w0+w1)\n\n    balanced_log_loss /= 2\n\n    \n\n    return balanced_log_loss","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:23:17.502701Z","iopub.execute_input":"2024-11-15T18:23:17.503095Z","iopub.status.idle":"2024-11-15T18:23:17.515606Z","shell.execute_reply.started":"2024-11-15T18:23:17.503052Z","shell.execute_reply":"2024-11-15T18:23:17.514353Z"},"papermill":{"duration":0.014742,"end_time":"2024-05-18T13:31:23.846432","exception":false,"start_time":"2024-05-18T13:31:23.83169","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_and_evaluate(model, X_train, y_train, X_test, y_test, best_score, best_model):\n    model.fit(X_train, y_train)\n    predictions = model.predict_proba(X_test)\n    score = balanced_log_loss(y_test, predictions)\n    if score < best_score:\n        return score, model\n    return best_score, best_model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T18:23:17.517282Z","iopub.execute_input":"2024-11-15T18:23:17.517669Z","iopub.status.idle":"2024-11-15T18:23:17.527322Z","shell.execute_reply.started":"2024-11-15T18:23:17.51763Z","shell.execute_reply":"2024-11-15T18:23:17.526118Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\nval_set = StratifiedKFold(n_splits=10, shuffle=True)\n# Initialize best models and scores\nbest_model_lgb, best_model_xg, best_model_cat, best_model_tabpf = None, None, None, None\nbest_score_lgb, best_score_xg, best_score_cat, best_score_tabpf = float('inf'), float('inf'), float('inf'), float('inf')\n    \nfor i in range(2, 12, 1):\n    lgb_model = lgb(max_depth=i, learning_rate=1e-2,metric = 'binary_logloss', n_estimators=100, verbose=-1, class_weight=\"balanced\")\n    xg_model = xgboost.XGBClassifier(n_estimators = 256, max_depth = i, scale_pos_weight=scale_pos)\n    catboost_model = CatBoostClassifier(iterations=1000, learning_rate=1e-2, depth=i, loss_function='Logloss', verbose=False, auto_class_weights = \"Balanced\")\n    Tabpf_model = tabpfn.TabPFNClassifier(N_ensemble_configurations=64, device='cpu')\n    \n    for train_index, test_index in val_set.split(train_df, test_df):\n        X_train, X_test = train_df[train_index], train_df[test_index]\n        y_train, y_test = test_df[train_index], test_df[test_index]\n    \n        # Evaluate each model using the helper function\n        best_score_lgb, best_model_lgb = train_and_evaluate(lgb_model, X_train, y_train, X_test, y_test, best_score_lgb, best_model_lgb)\n        best_score_xg, best_model_xg = train_and_evaluate(xg_model, X_train, y_train, X_test, y_test, best_score_xg, best_model_xg)\n        best_score_cat, best_model_cat = train_and_evaluate(catboost_model, X_train, y_train, X_test, y_test, best_score_cat, best_model_cat)\n        best_score_tabpf, best_model_tabpf = train_and_evaluate(Tabpf_model, X_train, y_train, X_test, y_test, best_score_tabpf, best_model_tabpf)\n\n# Final results\nprint(\"Final best LightGBM Model Score:\", best_score_lgb)\nprint(\"Final best XGBoost Model Score:\", best_score_xg)\nprint(\"Final best CatBoost Model Score:\", best_score_cat)\nprint(\"Final best TabPFN Model Score:\", best_score_tabpf)\n# print(\"Final best AdaBoost Model Score:\", best_score_ada)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-15T18:41:09.595772Z","iopub.execute_input":"2024-11-15T18:41:09.596239Z","iopub.status.idle":"2024-11-15T18:42:00.175536Z","shell.execute_reply.started":"2024-11-15T18:41:09.596197Z","shell.execute_reply":"2024-11-15T18:42:00.173133Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"> Helper function for fine tuning","metadata":{}},{"cell_type":"code","source":"def reweight(p,class_0_est_instances,others_est_instances):\n    new_p = p * np.array([[1/(class_0_est_instances if i==0 else others_est_instances) for i in range(p.shape[1])]])\n    return new_p / np.sum(new_p,axis=1,keepdims=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-16T14:27:26.328662Z","iopub.execute_input":"2024-11-16T14:27:26.329684Z","iopub.status.idle":"2024-11-16T14:27:26.371764Z","shell.execute_reply.started":"2024-11-16T14:27:26.329628Z","shell.execute_reply":"2024-11-16T14:27:26.370424Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ntest_df_pandas = pd.read_csv('/kaggle/input/icr-identify-age-related-conditions/test.csv')\n\nscale = StandardScaler().fit(test_df_1)\n\ntest_df_1 = scale.transform(test_df_1)\n\nlgb_pred = best_model_lgb.predict_proba(test_df_1)\n\nxg_pred = best_model_xg.predict_proba(test_df_1)\n\ncat_pred = best_model_cat.predict_proba(test_df_1)\n\ntabf_pred = best_model_tabpf.predict_proba(test_df_1)\n\ntotal_score = best_score_lgb + best_score_xg + best_score_cat + best_score_tabpf\n\n# Calculate total score\ntotal_score = best_score_lgb + best_score_xg + best_score_cat + best_score_tabpf\n\n# Calculate weights for each score\nlgb_w = 0.1 # best_score_lgb / total_score\nxg_w = 0.1 # best_score_xg / total_score\ncat_w =0.25 # best_score_cat / total_score\ntabpf_w =0.55 # best_score_tabpf / total_score\n\n# Calculate weighted predictions dynamically\npredictions = (lgb_pred * lgb_w +\n               xg_pred * xg_w +\n               cat_pred * cat_w +\n               tabf_pred * tabpf_w)\n\ninst0=predictions[:,0].sum()\ninst1=predictions[:,1].sum()\npredictions=reweight(predictions,inst0,inst1)\n\ndf = pd.DataFrame({'Id': test_df_pandas.Id, 'class_0': predictions[:,0], 'class_1': predictions[:,1]})\n\ndf.to_csv('/kaggle/working/submission.csv', index=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-11-15T18:23:57.693118Z","iopub.status.idle":"2024-11-15T18:23:57.693623Z","shell.execute_reply.started":"2024-11-15T18:23:57.693395Z","shell.execute_reply":"2024-11-15T18:23:57.69342Z"},"papermill":{"duration":140.408243,"end_time":"2024-05-18T13:33:44.258618","exception":false,"start_time":"2024-05-18T13:31:23.850375","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null}]}